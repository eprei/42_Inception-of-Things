= My notes

== Glossary

VXLAN:: https://en.wikipedia.org/wiki/Virtual_Extensible_LAN[Virtual Extensible LAN]

== Documentation

. Launch machines
+
----
vagrant up
----

== Cheat sheet

=== Ingress

Useful commands to check the status of the “ingress controller”

Check the status of the pods to see if the ingress controller is online.
+
----
kubectl get pods --namespace ingress-nginx
----

Check to see if the service is online. This of type NodePort, so do you have an EXTERNAL-IP?
+
----
kubectl get services --namespace ingress-nginx
----

Check out the ingressclass nginx.
Ingresses we will need specify an ingressclassname
+
----
kubectl describe ingressclasses nginx
----

Get the status of the ingress.
+
----
kubectl get ingress --watch # Wait for the Address of intress-path to be populated before proceeding
kubectl get services --namespace ingress-nginx
kubectl describe ingress
----

== Journal

=== Try to use qemu instead of virtual box

. Replace virtualbox with qemu in `p1/Vagrantfile`
+
[source, patch]
----
    ...
-   control.vm.provider "virtualbox" do |vb|
-           vb.memory = "512"
+   control.vm.provider "qemu" do |qe|
+           qe.memory = "512"
    ...
----

. Install libvirt plugin with vagrant
+
----
vagrant plugin install vagrant-libvrt
vagrant plugin install libvrt
----
+
but I receive this error message
+
----
Vagrant failed to load a configured plugin source. This can be caused
by a variety of issues including: transient connectivity issues, proxy
filtering rejecting access to a configured plugin source, or a configured
plugin source not responding correctly. Please review the error message
below to help resolve the issue:

  Net::OpenTimeout: Failed to open TCP connection to gems.hashicorp.com:443 (execution expired) (https://gems.hashicorp.com/specs.4.8.gz)

Source: https://gems.hashicorp.com/
----

=== Install virtualbox on archlinux

Install "virtualbox-host-modules-arch" for my Arch Linux

Install virtualbox plugin
+
----
vagrant plugin install virtualbox
----
+
I receive this
+
----
The provider 'virtualbox' that was requested to back the machine
'epresa-cS' is reporting that it isn't usable on this system. The
reason is shown below:

VirtualBox is complaining that the kernel module is not loaded. Please
run `VBoxManage --version` or open the VirtualBox GUI to see the error
message which should contain instructions on how to fix this error.
----

I reboot my computer to load the `vboxdrv` kernel module

=== Launch vagrant

----
vagrant up
----
+
----
$ vagrant up
Bringing machine 'epresa-cS' up with 'virtualbox' provider...
Bringing machine 'epresa-cSW' up with 'virtualbox' provider...
==> epresa-cS: Box 'centos/7' could not be found. Attempting to find and install...
    epresa-cS: Box Provider: virtualbox
    epresa-cS: Box Version: >= 0
==> epresa-cS: Loading metadata for box 'centos/7'
    epresa-cS: URL: https://vagrantcloud.com/api/v2/vagrant/centos/7
==> epresa-cS: Adding box 'centos/7' (v2004.01) for provider: virtualbox
    epresa-cS: Downloading: https://vagrantcloud.com/centos/boxes/7/versions/2004.01/providers/virtualbox/unknown/vagrant.box
Download redirected to host: cloud.centos.org
    epresa-cS: Calculating and comparing box checksum...
==> epresa-cS: Successfully added box 'centos/7' (v2004.01) for 'virtualbox'!
==> epresa-cS: Preparing master VM for linked clones...
    epresa-cS: This is a one time operation. Once the master VM is prepared,
    epresa-cS: it will be used as a base for linked clones, making the creation
    epresa-cS: of new VMs take milliseconds on a modern system.
==> epresa-cS: Importing base box 'centos/7'...
==> epresa-cS: Cloning VM...
==> epresa-cS: Matching MAC address for NAT networking...
==> epresa-cS: Checking if box 'centos/7' version '2004.01' is up to date...
==> epresa-cS: Setting the name of the VM: Server
==> epresa-cS: Clearing any previously set network interfaces...
==> epresa-cS: Preparing network interfaces based on configuration...
    epresa-cS: Adapter 1: nat
    epresa-cS: Adapter 2: hostonly
==> epresa-cS: Forwarding ports...
    epresa-cS: 22 (guest) => 2222 (host) (adapter 1)
==> epresa-cS: Running 'pre-boot' VM customizations...
==> epresa-cS: Booting VM...
==> epresa-cS: Waiting for machine to boot. This may take a few minutes...
    epresa-cS: SSH address: 127.0.0.1:2222
    epresa-cS: SSH username: vagrant
    epresa-cS: SSH auth method: private key
    epresa-cS:
    epresa-cS: Vagrant insecure key detected. Vagrant will automatically replace
    epresa-cS: this with a newly generated keypair for better security.
    epresa-cS:
    epresa-cS: Inserting generated public key within guest...
    epresa-cS: Removing insecure key from the guest if it's present...
    epresa-cS: Key inserted! Disconnecting and reconnecting using new SSH key...
==> epresa-cS: Machine booted and ready!
==> epresa-cS: Checking for guest additions in VM...
    epresa-cS: No guest additions were detected on the base box for this VM! Guest
    epresa-cS: additions are required for forwarded ports, shared folders, host only
    epresa-cS: networking, and more. If SSH fails on this machine, please install
    epresa-cS: the guest additions and repackage the box to continue.
    epresa-cS:
    epresa-cS: This is not an error message; everything may continue to work properly,
    epresa-cS: in which case you may ignore this message.
==> epresa-cS: Setting hostname...
==> epresa-cS: Configuring and enabling network interfaces...
==> epresa-cS: Running provisioner: shell...
    epresa-cS: Running: /tmp/vagrant-shell20240817-2566-lsaow1.sh
    epresa-cS: bootstrapping server...
==> epresa-cSW: Box 'centos/7' could not be found. Attempting to find and install...
    epresa-cSW: Box Provider: virtualbox
    epresa-cSW: Box Version: >= 0
==> epresa-cSW: Loading metadata for box 'centos/7'
    epresa-cSW: URL: https://vagrantcloud.com/api/v2/vagrant/centos/7
==> epresa-cSW: Adding box 'centos/7' (v2004.01) for provider: virtualbox
==> epresa-cSW: Cloning VM...
==> epresa-cSW: Matching MAC address for NAT networking...
==> epresa-cSW: Checking if box 'centos/7' version '2004.01' is up to date...
==> epresa-cSW: Setting the name of the VM: ServerWorker
==> epresa-cSW: Fixed port collision for 22 => 2222. Now on port 2200.
==> epresa-cSW: Clearing any previously set network interfaces...
==> epresa-cSW: Preparing network interfaces based on configuration...
    epresa-cSW: Adapter 1: nat
    epresa-cSW: Adapter 2: hostonly
==> epresa-cSW: Forwarding ports...
    epresa-cSW: 22 (guest) => 2200 (host) (adapter 1)
==> epresa-cSW: Running 'pre-boot' VM customizations...
==> epresa-cSW: Booting VM...
==> epresa-cSW: Waiting for machine to boot. This may take a few minutes...
    epresa-cSW: SSH address: 127.0.0.1:2200
    epresa-cSW: SSH username: vagrant
    epresa-cSW: SSH auth method: private key
    epresa-cSW:
    epresa-cSW: Vagrant insecure key detected. Vagrant will automatically replace
    epresa-cSW: this with a newly generated keypair for better security.
    epresa-cSW:
    epresa-cSW: Inserting generated public key within guest...
    epresa-cSW: Removing insecure key from the guest if it's present...
    epresa-cSW: Key inserted! Disconnecting and reconnecting using new SSH key...
==> epresa-cSW: Machine booted and ready!
==> epresa-cSW: Checking for guest additions in VM...
    epresa-cSW: No guest additions were detected on the base box for this VM! Guest
    epresa-cSW: additions are required for forwarded ports, shared folders, host only
    epresa-cSW: networking, and more. If SSH fails on this machine, please install
    epresa-cSW: the guest additions and repackage the box to continue.
    epresa-cSW:
    epresa-cSW: This is not an error message; everything may continue to work properly,
    epresa-cSW: in which case you may ignore this message.
==> epresa-cSW: Setting hostname...
==> epresa-cSW: Configuring and enabling network interfaces...
==> epresa-cSW: Running provisioner: shell...
    epresa-cSW: Running: /tmp/vagrant-shell20240817-2566-2sojqr.sh
    epresa-cSW: bootstrapping serverWorker...
----

=== Add an environment variable to the vagrantfile

Resource: https://www.ryanchapin.com/using-environment-variables-in-a-vagrant-file/[Using Environment Variables in a Vagrant File]

[source,patch]
----
-  control.vm.hostname =  ENV['WHOAMI']
+  control.vm.hostname = "epresa-cS"
----

Add environment variable

----
$ echo $WHOAMI
epresa-cS
----

But got this after `vagrant up`

----
Vagrant failed to initialize at a very early stage:

There was an error loading a Vagrantfile. The file being loaded
and the error message are shown below. This is usually caused by
an invalid or undefined variable.

Path: /opt/vagrant/embedded/gems/gems/vagrant-2.4.1/plugins/kernel_v2/config/vm.rb
Line number: 0
Message: undefined method `to_sym'
----

=== Connect over SSH

Launch

----
vagrant up
----

Connect with

----
vagrant ssh epresa-cS
vagrant ssh epresa-cSW
----

=== Get the sshd config

After to be connected to the machine, I print the sshd config

----
[vagrant@epresa-cS ~]$ sudo cat  /etc/ssh/sshd_config  | grep -v "^#" | grep -v "^$"
HostKey /etc/ssh/ssh_host_rsa_key
HostKey /etc/ssh/ssh_host_ecdsa_key
HostKey /etc/ssh/ssh_host_ed25519_key
SyslogFacility AUTHPRIV
AuthorizedKeysFile      .ssh/authorized_keys
PasswordAuthentication no
ChallengeResponseAuthentication no
GSSAPIAuthentication yes
GSSAPICleanupCredentials no
UsePAM yes
X11Forwarding yes
UseDNS no
AcceptEnv LANG LC_CTYPE LC_NUMERIC LC_TIME LC_COLLATE LC_MONETARY LC_MESSAGES
AcceptEnv LC_PAPER LC_NAME LC_ADDRESS LC_TELEPHONE LC_MEASUREMENT
AcceptEnv LC_IDENTIFICATION LC_ALL LANGUAGE
AcceptEnv XMODIFIERS
Subsystem       sftp    /usr/libexec/openssh/sftp-server
----

And print the public ssh key

----
[vagrant@epresa-cS ~]$ cat ~/.ssh/authorized_keys
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIOWG7FyvvixzfNOgf6pR2IgNqjtBlGO06bnVAmTIp/XD vagrant
----

It's interresting, because we can understand that vagrant create automaticly ssh keys for the connection

=== Setup a VPS

I discover a nice feature, https://en.wikipedia.org/wiki/Wake-on-LAN[Wake-on-LAN]

resouce: https://wiki.archlinux.org/title/Wake-on-LAN#On_the_same_LAN[arch wiki]

Got my mate public key with

----
curl https://github.com/${USERNAME}.keys
----

I choosed Arch linux as VPS ditribution because I am use to with this one.
And I want to know what is the best between arch and debian...

=== VPS user

----
[theo@iot ~]$ vagrant plugin install virtualbox
Installing the 'virtualbox' plugin. This can take a few minutes...
Vagrant failed to properly resolve required dependencies. These
errors can commonly be caused by misconfigured plugin installations
or transient network issues. The reported error is:

conflicting dependencies bigdecimal (= 3.1.3) and bigdecimal (= 3.1.8)
  Activated bigdecimal-3.1.8
  which does not match conflicting dependency (= 3.1.3)

  Conflicting dependency chains:
    bigdecimal (= 3.1.8), 3.1.8 activated

  versus:
    bigdecimal (= 3.1.3)

  Gems matching bigdecimal (= 3.1.3):
    bigdecimal-3.1.3
----

Configure the firewall to avoid trouble from my mates
(avoid any ping to other devices)

[source, bash]
----
ufw deny out to 192.168.1.0/24 # my LAN
ufw allow out to 192.168.1.1 # gateway
ufw default allow outgoing # get access to the web
----

=== Try to install k3s on CenOS

----
[vagrant@epresa-cS ~]$ curl -sfL https://get.k3s.io | sh -
[INFO]  Finding release for channel stable
[INFO]  Using v1.30.3+k3s1 as release
[INFO]  Downloading hash https://github.com/k3s-io/k3s/releases/download/v1.30.3+k3s1/sha256sum-amd64.txt
[INFO]  Downloading binary https://github.com/k3s-io/k3s/releases/download/v1.30.3+k3s1/k3s
[INFO]  Verifying binary download
[INFO]  Installing k3s to /usr/local/bin/k3s
[INFO]  Finding available k3s-selinux versions
Loaded plugins: fastestmirror
Determining fastest mirrors
Could not retrieve mirrorlist http://mirrorlist.centos.org/?release=7&arch=x86_64&repo=os&infra=vag error was
14: curl#6 - "Could not resolve host: mirrorlist.centos.org; Unknown error"


 One of the configured repositories failed (Unknown),
 and yum doesn't have enough cached data to continue. At this point the only
 safe thing yum can do is fail. There are a few ways to work "fix" this:

     1. Contact the upstream for the repository and get them to fix the problem.

     2. Reconfigure the baseurl/etc. for the repository, to point to a working
        upstream. This is most often useful if you are using a newer
        distribution release than is supported by the repository (and the
        packages for the previous distribution release still work).

     3. Run the command with the repository temporarily disabled
            yum --disablerepo=<repoid> ...

     4. Disable the repository permanently, so yum won't use it by default. Yum
        will then just ignore the repository until you permanently enable it
        again or use --enablerepo for temporary usage:

            yum-config-manager --disable <repoid>
        or
            subscription-manager repos --disable=<repoid>

     5. Configure the failing repository to be skipped, if it is unavailable.
        Note that yum will try to contact the repo. when it runs most commands,
        so will have to try and fail each time (and thus. yum will be be much
        slower). If it is a very temporary problem though, this is often a nice
        compromise:

            yum-config-manager --save --setopt=<repoid>.skip_if_unavailable=true

Cannot find a valid baseurl for repo: base/7/x86_64
----

CenOS is yet deprecated (see https://rockylinux.org/about[rocky linux about]) so we changed for Debian.

=== K3S : Server and Agent

____
Resource: https://docs.k3s.io/installation/configuration#configuration-with-install-script[Configuration with install script]
____

First machine (Server)

----
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="server" sh -s - --flannel-backend none --token "mypassword"
----

Second machine (ServerWorker)

----
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="agent" K3S_TOKEN="mypassword" sh -s - --server https://192.168.56.110
----

=== Troubleshooting: vagrant up take times

____
resource: robin gave me this https://youtu.be/JLnjMCRLcCo?t=240[video and time]
____

The first machine (server) has good installed, but the second (agent) take much time.

. I connect to the agent with ssh
. Kill the k3s process
+
----
sudo kill $(pgrep k3s)
----

. Check the command (value of `ExecStart=`) from `/etc/systemd/system/k3s-agent.service`
+
----
tail /etc/systemd/system/k3s-agent.service
----

. Run it
+
----
vagrant@theoSW:~$ sudo /usr/local/bin/k3s agent '--server' 'https://192.168.56.110' --flannel-iface 'eth1' --token mypassword
INFO[0000] Found ip 192.168.56.111 from iface eth1
INFO[0000] Starting k3s agent v1.30.3+k3s1 (f6466040)
INFO[0000] Adding server to load balancer k3s-agent-load-balancer: 192.168.56.110:443
INFO[0000] Running load balancer k3s-agent-load-balancer 127.0.0.1:6444 -> [192.168.56.110:443] [default: 192.168.56.110:443]
ERRO[0000] failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": read tcp 127.0.0.1:42446->127.0.0.1:6444: read: connection reset by peer
...
ERRO[0004] failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": read tcp 127.0.0.1:52174->127.0.0.1:6444: read: connection reset by peer
----

. Run it with debug flag
+
----
vagrant@theoSW:~$ sudo /usr/local/bin/k3s agent '--server' 'https://192.168.56.110' --flannel-iface 'eth1' --token mypassword --debug
INFO[0000] Found ip 192.168.56.111 from iface eth1
INFO[0000] Starting k3s agent v1.30.3+k3s1 (f6466040)
INFO[0000] Adding server to load balancer k3s-agent-load-balancer: 192.168.56.110:443
INFO[0000] Running load balancer k3s-agent-load-balancer 127.0.0.1:6444 -> [192.168.56.110:443] [default: 192.168.56.110:443]
DEBU[0000] Supervisor proxy using supervisor=https://127.0.0.1:6444 apiserver=https://127.0.0.1:6444 lb=true
DEBU[0000] Dial error from load balancer k3s-agent-load-balancer after 560.311µs: dial tcp 192.168.56.110:443: connect: connection refused
DEBU[0000] Health checks for all servers in load balancer k3s-agent-load-balancer have failed: retrying with health checks ignored
DEBU[0000] Dial error from load balancer k3s-agent-load-balancer after 642.917µs: dial tcp 192.168.56.110:443: connect: connection refused
DEBU[0000] Incoming conn 127.0.0.1:43070, error dialing load balancer servers: all servers failed
ERRO[0000] failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": EOF
----

. Connect to the server and try to precise the `eth1` as nic with flannel-iface's flag
+
----
vagrant@theoS:~$ sudo /usr/local/bin/k3s server '--flannel-backend' 'none' '--token' 'mypassword' --flannel-iface 'eth1'
INFO[0000] Found ip 192.168.56.110 from iface eth1
INFO[0000] Starting k3s v1.30.3+k3s1 (f6466040)
INFO[0000] Configuring sqlite3 database connection pooling: maxIdleConns=2, maxOpenConns=0, connMaxLifetime=0s
INFO[0000] Configuring database table schema and indexes, this may take a moment...
INFO[0000] Database tables and indexes are up to date
INFO[0000] Kine available at unix://kine.sock
INFO[0000] Reconciling bootstrap data between datastore and disk
INFO[0000] certificate CN=k3s,O=k3s signed by CN=k3s-server-ca@1724327973: notBefore=2024-08-22 11:59:33 +0000 UTC notAfter=2025-08-22 12:35:50 +0000 UTC
INFO[0000] Running kube-apiserver --advertise-address=192.168.56.110 --advertise-port=6443 --allow-privileged=true --anonymous-auth=false --api-audiences=https://kubernetes.default.svc.cluster.local,k3s --authorization-mode=Node,RBAC --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --egress-selector-config-file=/var/lib/rancher/k3s/server/etc/egress-selector-config.yaml --enable-admission-plugins=NodeRestriction --enable-aggregator-routing=true --enable-bootstrap-token-auth=true --etcd-servers=unix://kine.sock --kubelet-certificate-authority=/var/lib/rancher/k3s/server/tls/server-ca.crt --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --profiling=false --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --service-node-port-range=30000-32767 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key
INFO[0000] Running kube-scheduler --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --leader-elect=false --profiling=false --secure-port=10259
INFO[0000] Running kube-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-kube-apiserver-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kube-apiserver-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-client-cert-file=/var/lib/rancher/k3s/server/tls/client-ca.nochain.crt --cluster-signing-kubelet-client-key-file=/var/lib/rancher/k3s/server/tls/client-ca.key --cluster-signing-kubelet-serving-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-kubelet-serving-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --cluster-signing-legacy-unknown-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.nochain.crt --cluster-signing-legacy-unknown-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --configure-cloud-routes=false --controllers=*,tokencleaner,-service,-route,-cloud-node-lifecycle --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --leader-elect=false --profiling=false --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=10257 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.current.key --service-cluster-ip-range=10.43.0.0/16 --use-service-account-credentials=true
INFO[0000] Waiting for API server to become available
I0822 12:35:50.319443    2535 options.go:221] external host was not specified, using 192.168.56.110
I0822 12:35:50.324932    2535 server.go:156] Version: v1.30.3+k3s1
I0822 12:35:50.325384    2535 server.go:158] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0822 12:35:50.362436    2535 plugins.go:157] Loaded 12 mutating admission controller(s) successfully in the following order: NamespaceLifecycle,LimitRanger,ServiceAccount,NodeRestriction,TaintNodesByCondition,Priority,DefaultTolerationSeconds,DefaultStorageClass,StorageObjectInUseProtection,RuntimeClass,DefaultIngressClass,MutatingAdmissionWebhook.
I0822 12:35:50.371712    2535 shared_informer.go:313] Waiting for caches to sync for node_authorizer
INFO[0000] Running cloud-controller-manager --allocate-node-cidrs=true --authentication-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --authorization-kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --bind-address=127.0.0.1 --cloud-config=/var/lib/rancher/k3s/server/etc/cloud-config.yaml --cloud-provider=k3s --cluster-cidr=10.42.0.0/16 --configure-cloud-routes=false --controllers=*,-route --feature-gates=CloudDualStackNodeIPs=true --kubeconfig=/var/lib/rancher/k3s/server/cred/cloud-controller.kubeconfig --leader-elect=false --leader-elect-resource-name=k3s-cloud-controller-manager --node-status-update-frequency=1m0s --profiling=false
I0822 12:35:50.371757    2535 shared_informer.go:313] Waiting for caches to sync for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0822 12:35:50.372801    2535 plugins.go:160] Loaded 13 validating admission controller(s) successfully in the following order: LimitRanger,ServiceAccount,PodSecurity,Priority,PersistentVolumeClaimResize,RuntimeClass,CertificateApproval,CertificateSigning,ClusterTrustBundleAttest,CertificateSubjectRestriction,ValidatingAdmissionPolicy,ValidatingAdmissionWebhook,ResourceQuota.
I0822 12:35:50.388170    2535 instance.go:299] Using reconciler: lease
INFO[0000] Server node token is available at /var/lib/rancher/k3s/server/token
INFO[0000] To join server node to cluster: k3s server -s https://10.0.2.15:6443 -t ${SERVER_NODE_TOKEN}
INFO[0000] Agent node token is available at /var/lib/rancher/k3s/server/agent-token
INFO[0000] To join agent node to cluster: k3s agent -s https://10.0.2.15:6443 -t ${AGENT_NODE_TOKEN}
INFO[0000] Wrote kubeconfig /etc/rancher/k3s/k3s.yaml
INFO[0000] Run: k3s kubectl
I0822 12:35:50.418509    2535 handler.go:286] Adding GroupVersion apiextensions.k8s.io v1 to ResourceManager
W0822 12:35:50.418856    2535 genericapiserver.go:733] Skipping API apiextensions.k8s.io/v1beta1 because it has no resources.
INFO[0001] Waiting to retrieve agent configuration; server is not ready: starting
I0822 12:35:51.221117    2535 handler.go:286] Adding GroupVersion  v1 to ResourceManager
I0822 12:35:51.277037    2535 instance.go:696] API group "internal.apiserver.k8s.io" is not enabled, skipping.
W0822 12:35:51.363648    2535 reflector.go:547] storage/cacher.go:/horizontalpodautoscalers: failed to list *autoscaling.HorizontalPodAutoscaler: rpc error: code = Unavailable desc = error reading from server: EOF
E0822 12:35:51.363719    2535 cacher.go:475] cacher (horizontalpodautoscalers.autoscaling): unexpected ListAndWatch error: failed to list *autoscaling.HorizontalPodAutoscaler: rpc error: code = Unavailable desc = error reading from server: EOF; reinitializing...
INFO[0011] Password verified locally for node theos
INFO[0011] certificate CN=theos signed by CN=k3s-server-ca@1724327973: notBefore=2024-08-22 11:59:33 +0000 UTC notAfter=2025-08-22 12:36:01 +0000 UTC
INFO[0012] certificate CN=system:node:theos,O=system:nodes signed by CN=k3s-client-ca@1724327973: notBefore=2024-08-22 11:59:33 +0000 UTC notAfter=2025-08-22 12:36:01 +0000 UTC
INFO[0012] Module overlay was already loaded
INFO[0012] Module nf_conntrack was already loaded
INFO[0012] Module br_netfilter was already loaded
INFO[0012] Module iptable_nat was already loaded
INFO[0012] Module iptable_filter was already loaded
INFO[0012] Logging containerd to /var/lib/rancher/k3s/agent/containerd/containerd.log
INFO[0012] Running containerd -c /var/lib/rancher/k3s/agent/etc/containerd/config.toml -a /run/k3s/containerd/containerd.sock --state /run/k3s/containerd --root /var/lib/rancher/k3s/agent/containerd
INFO[0013] containerd is now running
INFO[0013] Creating k3s-cert-monitor event broadcaster
INFO[0013] Connecting to proxy                           url="wss://127.0.0.1:6443/v1-k3s/connect"
INFO[0013] Running kubelet --address=0.0.0.0 --allowed-unsafe-sysctls=net.ipv4.ip_forward,net.ipv6.conf.all.forwarding --anonymous-auth=false --authentication-token-webhook=true --authorization-mode=Webhook --cgroup-driver=cgroupfs --client-ca-file=/var/lib/rancher/k3s/agent/client-ca.crt --cloud-provider=external --cluster-dns=10.43.0.10 --cluster-domain=cluster.local --container-runtime-endpoint=unix:///run/k3s/containerd/containerd.sock --containerd=/run/k3s/containerd/containerd.sock --eviction-hard=imagefs.available<5%,nodefs.available<5% --eviction-minimum-reclaim=imagefs.available=10%,nodefs.available=10% --fail-swap-on=false --feature-gates=CloudDualStackNodeIPs=true --healthz-bind-address=127.0.0.1 --hostname-override=theos --kubeconfig=/var/lib/rancher/k3s/agent/kubelet.kubeconfig --kubelet-cgroups=/k3s --node-ip=192.168.56.110 --node-labels= --pod-manifest-path=/var/lib/rancher/k3s/agent/pod-manifests --read-only-port=0 --resolv-conf=/etc/resolv.conf --serialize-image-pulls=false --tls-cert-file=/var/lib/rancher/k3s/agent/serving-kubelet.crt --tls-private-key-file=/var/lib/rancher/k3s/agent/serving-kubelet.key
INFO[0013] Handling backend connection request [theos]
INFO[0013] Remotedialer connected to proxy               url="wss://127.0.0.1:6443/v1-k3s/connect"
ERRO[0013] Sending HTTP 503 response to 127.0.0.1:58444: runtime core not ready
INFO[0013] Running kube-proxy --cluster-cidr=10.42.0.0/16 --conntrack-max-per-core=0 --conntrack-tcp-timeout-close-wait=0s --conntrack-tcp-timeout-established=0s --healthz-bind-address=127.0.0.1 --hostname-override=theos --kubeconfig=/var/lib/rancher/k3s/agent/kubeproxy.kubeconfig --proxy-mode=iptables
E0822 12:36:03.526041    2535 server.go:1051] "Failed to retrieve node info" err="apiserver not ready"
E0822 12:36:04.696468    2535 server.go:1051] "Failed to retrieve node info" err="apiserver not ready"
E0822 12:36:07.053314    2535 server.go:1051] "Failed to retrieve node info" err="apiserver not ready"
E0822 12:36:11.095738    2535 server.go:1051] "Failed to retrieve node info" err="apiserver not ready"
Error: problem initializing API group "batch" : context deadline exceeded
ERRO[0021] apiserver exited: problem initializing API group "batch" : context deadline exceeded
----

NOTE: Environment file are in `/etc/systemd/system/k3s.service.env` or in `/etc/systemd/system/k3s-agent.service.env`

. Try to curl
+
----
vagrant@theoSW:~$ curl https://127.0.0.1:6444/cacerts
curl: (60) SSL certificate problem: self-signed certificate
More details here: https://curl.se/docs/sslcerts.html

curl failed to verify the legitimacy of the server and therefore could not
establish a secure connection to it. To learn more about this situation and
how to fix it, please visit the web page mentioned above.
vagrant@theoSW:~$ curl --insecure https://127.0.0.1:6444/cacerts
404 page not found
vagrant@theoSW:~$ curl --insecure https://127.0.0.1:6445/index.html
curl: (7) Failed to connect to 127.0.0.1 port 6445 after 0 ms: Couldn't connect to server
----

I think that `cacerts` does not exist

----
vagrant@theoSW:~$ curl --insecure https://192.168.56.110:6443/cacerts
-----BEGIN CERTIFICATE-----
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
-----END CERTIFICATE-----
----

I retried with port in server flag

----
vagrant@theoSW:~$ sudo /usr/local/bin/k3s agent '--server' 'https://192.168.56.110:6443' --flannel-iface 'eth1' --token mypassword --debug
INFO[0000] Found ip 192.168.56.111 from iface eth1
INFO[0000] Starting k3s agent v1.30.3+k3s1 (f6466040)
INFO[0000] Adding server to load balancer k3s-agent-load-balancer: 192.168.56.110:6443
INFO[0000] Running load balancer k3s-agent-load-balancer 127.0.0.1:6444 -> [192.168.56.110:6443] [default: 192.168.56.110:6443]
DEBU[0000] Supervisor proxy using supervisor=https://127.0.0.1:6444 apiserver=https://127.0.0.1:6444 lb=true
ERRO[0000] failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": read tcp 127.0.0.1:36370->127.0.0.1:6444: read: connection reset by peer
WARN[0002] Cluster CA certificate is not trusted by the host CA bundle, but the token does not include a CA hash. Use the full token from the server's node-token file to enable Cluster CA validation.
INFO[0002] Waiting to retrieve agent configuration; server is not ready: CA cert validation failed: Get "https://127.0.0.1:6444/cacerts": EOF
INFO[0009] Waiting to retrieve agent configuration; server is not ready: Get "https://127.0.0.1:6444/v1-k3s/client-ca.crt": read tcp 127.0.0.1:37912->127.0.0.1:6444: read: connection reset by peer
INFO[0035] Waiting to retrieve agent configuration; server is not ready: failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
INFO[0042] Waiting to retrieve agent configuration; server is not ready: failed to get CA certs: Get "https://127.0.0.1:6444/cacerts": read tcp 127.0.0.1:50090->127.0.0.1:6444: read: connection reset by peer
----

=== Try steps manualy

. Remove commands in shell script provision.

. Manipulate server manually
+
----
sudo apt update ; sudo apt install -y curl
sudo curl -Lo /usr/local/bin/k3s https://github.com/k3s-io/k3s/releases/download/v1.26.5+k3s1/k3s; sudo chmod a+x /usr/local/bin/k3s
sudo K3S_KUBECONFIG_MODE="644" K3S_TOKEN="mypassword" /usr/local/bin/k3s server --flannel-backend none --flannel-iface "eth1"
----
+
I receive at the end this error:
+
----
E0822 14:07:19.908083    2173 kubelet.go:2475] "Container runtime network not ready" networkReady="NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized"
----

=== How access to a vagrant service

pf:: port forwarding

Kubernetes is flexible enought that you can select your provider.

For example, I use Virtual Box, so I need to configure port forwarding with virtual box

public_ip -router pf-> private_ip_server
private_ip_server -cloud provider pf-> private_ip_kube

=== Ingress file

Ingress need a controller, for example https://kubernetes.github.io/ingress-nginx/user-guide/basic-usage/[ingress-nginx].

== Meetings

=== Cyber reunion

____
2024-08-20: 12h00 on discord
____

Why CentOS ?

No particular reasons

tgrivel want to setup a VPS at home for working

== Resources

The Kubernetes Book (Nigel Poulton) : https://github.com/rohitg00/DevOps_Books/blob/main/The%20Kubernetes%20Book%20(Nigel%20Poulton)%20(z-lib.org).pdf

Kubernetes Crash Course for Absolute Beginners : https://youtu.be/s_o8dwzRlu4?si=bZlrSVcJXb9QpOOD 

Vagrant documentation : https://developer.hashicorp.com/vagrant/docs

Vagrant + K3s (vidéo en français) : https://www.youtube.com/watch?v=JLnjMCRLcCo

Vagrant cheat sheet : https://gist.github.com/wpscholar/a49594e2e2b918f4d0c4 
Pluralsight course : "Getting started with Kubernetes" 

La vidéo 04 06 présente un exemple de déploiement d'une application sur un cluster kubernetes via argo cd. La vidéo 04 02 parle également de l'interaction entre k8s et argo cd.
Kubernetes Playground : https://labs.play-with-k8s.com/ 
https://www.youtube.com/watch?v=JLnjMCRLcCo
https://portal.cloud.hashicorp.com/vagrant/discover?next=CgxXemN4TVRreU4xMD0%3D
HashiCorp Cloud Platform

=== Claude

==== what is the diference between k3s server and k3s agent ?

The main differences between a K3s server and a K3s agent are:

. Role:
** K3s server: Acts as the control plane, managing the cluster state and making global decisions.
** K3s agent: Runs workloads and communicates with the server to receive instructions.
. Components:
** K3s server: Runs etcd (or SQLite), API server, controller manager, and scheduler.
** K3s agent: Runs kubelet and kube-proxy.
. Deployment:
** K3s server: At least one is required in a cluster, can have multiple for high availability.
** K3s agent: Optional, used to add worker nodes to the cluster.
. Resource usage:
** K3s server: Generally requires more resources due to control plane components.
** K3s agent: Typically lighter on resource usage.
. Configuration:
** K3s server: Requires more configuration options for cluster management.
** K3s agent: Simpler configuration, mainly needs the server URL to join the cluster.

Would you like me to elaborate on any of these points or provide more information about K3s architecture?

==== but kubernetes has the same options?

You're right to ask about the comparison with standard Kubernetes. While K3s and standard Kubernetes share many concepts, there are some key differences:

. Simplification:
** K3s: Simplifies the server/agent distinction. A single binary can act as either server or agent.
** Kubernetes: Has a more complex architecture with separate components like kube-apiserver, kube-scheduler, etc.
. Resource footprint:
** K3s: Designed to be lightweight, with a smaller memory and CPU footprint.
** Kubernetes: Generally requires more resources, especially for the control plane.
. Installation and setup:
** K3s: Offers a simpler installation process, often with a single command.
** Kubernetes: Typically involves a more complex setup process.
. Default configurations:
** K3s: Comes with sensible defaults and built-in features like local storage provider.
** Kubernetes: Requires more manual configuration and add-ons.
. Use case focus:
** K3s: Targets edge computing, IoT, and resource-constrained environments.
** Kubernetes: Designed for a broader range of use cases, including large-scale deployments.

While K3s and Kubernetes share the server (control plane) and agent (worker) concepts, K3s simplifies this architecture to make it more accessible and lightweight. This is particularly useful for smaller deployments or resource-constrained environments.
